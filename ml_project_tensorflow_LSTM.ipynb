{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, GRU, LSTM, RNN, SpatialDropout1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the id column since the first column is the index anyways\n",
    "train = train.set_index('id', drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I set the max_features to 1500 because my computer couldn't train with all the features\n",
    "max_features = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some of the title and author values are not none so classify them as missing instead of null\n",
    "train[['title', 'author']] = train[['title', 'author']].fillna(value = 'Missing')\n",
    "\n",
    "#drop any other value that is null, this will only be rows where the text itself is null, so these values are not\n",
    "#needed\n",
    "train = train.dropna()\n",
    "train.isnull().sum()\n",
    "\n",
    "#count the length of each article by character and append that as a new column\n",
    "length = []\n",
    "[length.append(len(str(text))) for text in train['text']]\n",
    "train['length'] = length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "82                                                   \n",
       "169                                                  \n",
       "173                                   Guest   Guest  \n",
       "196            They got the heater turned up on high.\n",
       "295                                                  \n",
       "                             ...                     \n",
       "20350                         I hope nobody got hurt!\n",
       "20418                                 Guest   Guest  \n",
       "20431    \\nOctober 28, 2016 The Mothers by stclair by\n",
       "20513                                                \n",
       "20636                              Trump all the way!\n",
       "Name: text, Length: 207, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the articles where the length is less than 50 is mostly nonsense\n",
    "train['text'][train['length'] < 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop those articles\n",
    "train = train.drop(train['text'][train['length'] < 50].index, axis = 0)\n",
    "y = train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of ! in fake article:  0.437676364697869\n",
      "Mean of ! in true article:  0.1581200739515423\n"
     ]
    }
   ],
   "source": [
    "num_exclaim_true = sum(train['text'][train['label'] == 0].str.count('!')) / len(train['label'] == 0)\n",
    "num_exclaim_fake = sum(train['text'][train['label'] == 1].str.count('!')) / len(train['label'] == 1)\n",
    "print(\"Mean of ! in fake article: \", num_exclaim_fake)\n",
    "print(\"Mean of ! in true article: \", num_exclaim_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20554\n",
      "20554\n"
     ]
    }
   ],
   "source": [
    "print(len(train['label'] == 0))\n",
    "print(len(train['label'] == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of ! in fake article:  20735\n",
      "Mean of ! in true article:  36629\n"
     ]
    }
   ],
   "source": [
    "num_exclaim_true = sum(train['text'][train['label'] == 0].str.count('trump')) \n",
    "num_exclaim_fake = sum(train['text'][train['label'] == 1].str.count('trump')) \n",
    "print(\"Mean of ! in fake article: \", num_exclaim_fake)\n",
    "print(\"Mean of ! in true article: \", num_exclaim_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer for the text\n",
    "#num_words is the maximum number of words in the tokenizer, if an article has more than this number of unique \n",
    "#words than it drops the ones that haven't yet been seen\n",
    "#I used None to encode every word it sees\n",
    "\n",
    "#the oov_token tells the tokenizer what to do when it encounters a word that it hasn't seen before\n",
    "#this is useful in testing after the model has been trained, in a test article it will change the\n",
    "#word it hasn't seen befor to <00V> and then encode it, this will keep the length of the article \n",
    "#so it doesn't lose too much meaning\n",
    "\n",
    "#lower converts all the words to lowercase, splits the words by space\n",
    "\n",
    "#fit_on_texts looks at all the words in the dataset and uses that as the vocab to tokenize\n",
    "\n",
    "#the filter is the characters to ignore, I may be missing some as I look at the word_index dictionary\n",
    "#also note it is ignoring \".\" we may possibly want to encode some punctuation as it does carry some semantic\n",
    "#meaning? IDK, something to think about...\n",
    "\n",
    "#texts_to_sequences transforms the list of text to an encoding of each word that appears \n",
    "#(Example) text = [1, 3, 4, 2], internal structure of tekenizer contains a dictionary\n",
    "#dictionary:\n",
    "# word_index = {\n",
    "# 'hello' : 1,\n",
    "# 'you' : 2,\n",
    "# 'how' : 3,\n",
    "# 'are' : 4\n",
    "#}\n",
    "\n",
    "tokenizer = Tokenizer(num_words = None, \n",
    "                    filters='!\"\"-#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "                    lower = True, \n",
    "                    split = ' ',\n",
    "                    oov_token='<00V>')\n",
    "\n",
    "#for a baller computer\n",
    "# tokenizer = Tokenizer(num_words = None, \n",
    "#                     filters='!\"\"-#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "#                     lower = True, \n",
    "#                     split = ' ',\n",
    "#                     oov_token='<00V>')\n",
    "\n",
    "tokenizer.fit_on_texts(texts = train['text'])\n",
    "X = tokenizer.texts_to_sequences(texts = train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#key is word, value is encoding of that word\n",
    "#tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n",
      "130\n"
     ]
    }
   ],
   "source": [
    "#little explanation of X \n",
    "#first number in X is 130, which corresponds to the word 'house'\n",
    "print(X[0][0])\n",
    "print(tokenizer.word_index['house'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding the sequence, fills all articles with zeros at the beginning of the article to fit the length\n",
    "#of the largest sequence of words in the dataset\n",
    "X = pad_sequences(sequences = X, maxlen = max_features, padding = 'pre')\n",
    "\n",
    "#for a baller computer\n",
    "#X = pad_sequences(sequences = X, maxlen = None, padding = 'pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16443, 1500)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splits the data into train and test, note that this dataframe was only the testing data to begin with\n",
    "#so this split splits the test data further\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16443\n"
     ]
    }
   ],
   "source": [
    "#number of features for each article\n",
    "#my computer couldn't train with all the features, if you have a GPU version of tensorflow \n",
    "#or just an ungodly amount of RAM, set the input_dim = max_num_features and \n",
    "#uncomment all the places that are labeled with \"for a baller computer\"\n",
    "max_num_features = X_train.shape[0]\n",
    "print(max_num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ahhhh the good stuff\n",
    "#the input layer is the max num features, which is the number of words in the article with\n",
    "#the most number of words\n",
    "\n",
    "#ideally embedding will group similar articles together\n",
    "\n",
    "#then the first LSTM layer, the recurrent_dropout is the fraction of nodes around specific nodes to input\n",
    "#into this node\n",
    "\n",
    "#followed by a dropout layer, this randomly sets random nodes to a weight of zero, its meant to prevent\n",
    "#overfitting and somewhat more closely mimics how the brain learns \n",
    "#(certain neurons learn specific things over time and other neurons \"dropout\" or rewire if they are not\n",
    "#involved in a certain process)\n",
    "\n",
    "#then a dense layer with no dropout\n",
    "\n",
    "#then another dropout\n",
    "\n",
    "#final dense layer that is the output of the model, 0 for fake 1 for real\n",
    "lstm_model = Sequential(name = 'lstm_nn_model')\n",
    "lstm_model.add(layer = Embedding(input_dim = max_features, output_dim = 120, name = '1st_layer'))\n",
    "lstm_model.add(layer = LSTM(units = 120, dropout = 0.2, recurrent_dropout = 0.2, name = '2nd_layer'))\n",
    "lstm_model.add(layer = Dropout(rate = 0.5, name = '3rd_layer'))\n",
    "lstm_model.add(layer = Dense(units = 120,  activation = 'relu', name = '4th_layer'))\n",
    "lstm_model.add(layer = Dropout(rate = 0.5, name = '5th_layer'))\n",
    "lstm_model.add(layer = Dense(units = len(set(y)),  activation = 'sigmoid', name = 'output_layer'))\n",
    "\n",
    "# compiling the model\n",
    "#loss is objective function\n",
    "\n",
    "lstm_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "514/514 [==============================] - 798s 2s/step - loss: 0.3601 - accuracy: 0.8569\n",
      "Epoch 2/5\n",
      "514/514 [==============================] - 798s 2s/step - loss: 0.3523 - accuracy: 0.8505\n",
      "Epoch 3/5\n",
      "514/514 [==============================] - 776s 2s/step - loss: 0.2356 - accuracy: 0.9150\n",
      "Epoch 4/5\n",
      "514/514 [==============================] - 767s 1s/step - loss: 0.2392 - accuracy: 0.9088\n",
      "Epoch 5/5\n",
      "514/514 [==============================] - 784s 2s/step - loss: 0.2732 - accuracy: 0.8930\n"
     ]
    }
   ],
   "source": [
    "#WARNING training the model on my computer took half an hour to run the first time\n",
    "lstm_model_fit = lstm_model.fit(X_train, y_train, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
